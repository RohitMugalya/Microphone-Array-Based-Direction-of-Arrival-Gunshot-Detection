{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 44100\n",
    "N_MELS = 128\n",
    "TIME_STEPS = 128\n",
    "MIC_POSITIONS = torch.tensor([  # Hexagonal array (shape: [3, 6])\n",
    "    [0.0, 0.0, 0.0],      # Mic 1 (center)\n",
    "    [1.5, 0.0, 0.0],      # Mic 2 (right)\n",
    "    [0.75, 1.299, 0.0],   # Mic 3 (top-right)\n",
    "    [-0.75, 1.299, 0.0],  # Mic 4 (top-left)\n",
    "    [-1.5, 0.0, 0.0],     # Mic 5 (left)\n",
    "    [-0.75, -1.299, 0.0]  # Mic 6 (bottom-left)\n",
    "], dtype=torch.float32).T\n",
    "SPEED_OF_SOUND = 343.0\n",
    "DATA_DIR = \"../data/simulations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GunshotDataset(Dataset):\n",
    "    def __init__(self, sim_dirs, labels):\n",
    "        self.sim_dirs = sim_dirs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sim_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and convert to fixed-size spectrogram\n",
    "        specs = []\n",
    "        for mic in range(1, 7):\n",
    "            audio, _ = librosa.load(\n",
    "                f\"{self.sim_dirs[idx]}/mic_{mic}_recording.wav\",\n",
    "                sr=SAMPLE_RATE\n",
    "            )\n",
    "            S = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=SAMPLE_RATE,\n",
    "                n_mels=N_MELS,\n",
    "                n_fft=2048,\n",
    "                hop_length=len(audio)//(TIME_STEPS-1)\n",
    "            )\n",
    "            S = librosa.power_to_db(S, ref=np.max)\n",
    "            S = torch.tensor(S, dtype=torch.float32)\n",
    "            # Pad/truncate to 128x128\n",
    "            if S.shape[1] < TIME_STEPS:\n",
    "                S = torch.nn.functional.pad(S, (0, TIME_STEPS-S.shape[1]))\n",
    "            else:\n",
    "                S = S[:, :TIME_STEPS]\n",
    "            specs.append(S)\n",
    "        # Stack to [6, 128, 128] -> permute to [128, 128, 6]\n",
    "        X = torch.stack(specs).permute(1, 2, 0)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "# Load data\n",
    "labels = pd.read_csv(f\"{DATA_DIR}/labels.csv\")\n",
    "sim_dirs = [f\"{DATA_DIR}/gunshot_{i}\" for i in range(len(labels))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sim_dirs, labels[[\"distance\", \"azimuth\", \"elevation\"]].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = GunshotDataset(X_train, y_train)\n",
    "test_dataset = GunshotDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectogram PINN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input shape: [batch, 6, 128, 128]\n",
    "        self.conv1 = nn.Conv2d(6, 32, kernel_size=3, padding=1)  # [batch, 32, 128, 128]\n",
    "        self.pool = nn.MaxPool2d(2)  # [batch, 32, 64, 64]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # [batch, 64, 64, 64]\n",
    "        # Calculate flattened dimension: 64 * 64 * 64 = 262144\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Corrected dimension\n",
    "        self.fc2 = nn.Linear(128, 3)  # distance, azimuth, elevation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 128, 128, 6] -> permute to [batch, 6, 128, 128]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten to [batch, 64*64*64]\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physics Informed Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(y_pred, mic_positions=MIC_POSITIONS):\n",
    "    \"\"\"TDoA loss using predicted coordinates\"\"\"\n",
    "    distance, azimuth, elevation = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "    \n",
    "    # Convert to Cartesian (relative to mic1)\n",
    "    x = distance * torch.cos(azimuth) * torch.cos(elevation)\n",
    "    y = distance * torch.sin(azimuth) * torch.cos(elevation)\n",
    "    z = distance * torch.sin(elevation)\n",
    "    source_pos = torch.stack([x, y, z], dim=1)  # [batch, 3]\n",
    "    \n",
    "    # Expected TDoA\n",
    "    distances = torch.norm(mic_positions - source_pos.unsqueeze(2), dim=1)  # [batch, 6]\n",
    "    tdoa_pred = (distances - distances[:, 0:1]) / SPEED_OF_SOUND  # [batch, 6]\n",
    "    \n",
    "    # Approximate TDoA from spectrograms (simplified)\n",
    "    tdoa_spectro = ...  # Implement phase-based TDoA here\n",
    "    \n",
    "    return torch.mean((tdoa_pred[:, 1:] - tdoa_spectro)**2)\n",
    "\n",
    "def hybrid_loss(y_pred, y_true, alpha=0.1):\n",
    "    mse_loss = nn.functional.mse_loss(y_pred, y_true)\n",
    "    phys_loss = physics_loss(y_pred)\n",
    "    return mse_loss + alpha * phys_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x65536 and 262144x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=\u001b[32m5\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, epochs)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m      6\u001b[39m     optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     loss = hybrid_loss(y_pred, y_batch)\n\u001b[32m      9\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Microphone-Array-Based-Direction-of-Arrival-Gunshot-Detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Microphone-Array-Based-Direction-of-Arrival-Gunshot-Detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mSpectrogramPINN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     16\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool(torch.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     17\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten to [batch, 64*64*64]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m x = torch.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Microphone-Array-Based-Direction-of-Arrival-Gunshot-Detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Microphone-Array-Based-Direction-of-Arrival-Gunshot-Detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Microphone-Array-Based-Direction-of-Arrival-Gunshot-Detection/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x65536 and 262144x128)"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = hybrid_loss(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Initialize\n",
    "model = SpectrogramPINN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "# Train\n",
    "train(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
